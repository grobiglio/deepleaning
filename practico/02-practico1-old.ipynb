{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad92109e",
   "metadata": {},
   "source": [
    "# Práctico 1 - Parte 2\n",
    "\n",
    "[Enunciado](https://github.com/DiploDatos/AprendizajeProfundo/blob/master/Practico.md) del trabajo práctico.\n",
    "\n",
    "**Implementación de red neuronal [Perceptrón Multicapa](https://en.wikipedia.org/wiki/Multilayer_perceptron) (MLP).**\n",
    "\n",
    "[Documentación de Pytorch](https://pytorch.org/docs/stable/index.html)\n",
    "\n",
    "[Tutorial](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)\n",
    "\n",
    "## Integrantes\n",
    "- Mauricio Caggia\n",
    "- Luciano Monforte\n",
    "- Gustavo Venchiarutti\n",
    "- Guillermo Robiglio\n",
    "\n",
    "En esta segunda parte se cargan datos reducidos en la parte 1. Esto con el fin de optimizar memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d74bd8",
   "metadata": {},
   "source": [
    "## Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d08584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import bz2\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.parsing import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e78fb0",
   "metadata": {},
   "source": [
    "## Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fb3a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHIVO_SET_DE_ENTRENAMIENTO = './data/training_set.csv'\n",
    "ARCHIVO_SET_DE_ENTRENAMIENTO_REDUCIDO = './data/training_set_reduced.csv'\n",
    "ARCHIVO_SET_DE_PRUEBA = './data/test_set.csv'\n",
    "ARCHIVO_SET_DE_VALIDACION = './data/validation_set.csv'\n",
    "ARCHIVO_DE_EMBEDDINGS = './data/SBW-vectors-300-min5.txt.bz2'\n",
    "# ARCHIVO_DE_EMBEDDINGS = '../data/glove.6B.50d.txt.gz'\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fd7676",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93f9d6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.16 s, sys: 478 ms, total: 2.63 s\n",
      "Wall time: 2.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file_paths = [ARCHIVO_SET_DE_ENTRENAMIENTO,\n",
    "              ARCHIVO_SET_DE_ENTRENAMIENTO_REDUCIDO,\n",
    "              ARCHIVO_SET_DE_PRUEBA,\n",
    "              ARCHIVO_SET_DE_VALIDACION]\n",
    "i_train = 1\n",
    "i_test = 4\n",
    "df_train = pd.read_csv(file_paths[i_train])\n",
    "df_test = pd.read_csv(file_paths[i_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56c08a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>357743</th>\n",
       "      <td>Tijera Para El Cabello Filo  Navaja 5,5  P.car...</td>\n",
       "      <td>HAIRDRESSING_SCISSORS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442017</th>\n",
       "      <td>Cooler Cooler Master Hyper Tx3 Evo 1151 1150 A...</td>\n",
       "      <td>DESKTOP_COMPUTER_COOLERS_AND_FANS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097459</th>\n",
       "      <td>Lapicera Cactus X 50 U Merchandising Souvenirs...</td>\n",
       "      <td>PENS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547021</th>\n",
       "      <td>Mesa De Luz/cajonera Industrial - Hierro Y Madera</td>\n",
       "      <td>NIGHTSTANDS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681903</th>\n",
       "      <td>Seidio Superficie Caso Con El Metal Pata De Ca...</td>\n",
       "      <td>CELLPHONES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     title  \\\n",
       "357743   Tijera Para El Cabello Filo  Navaja 5,5  P.car...   \n",
       "442017   Cooler Cooler Master Hyper Tx3 Evo 1151 1150 A...   \n",
       "1097459  Lapicera Cactus X 50 U Merchandising Souvenirs...   \n",
       "547021   Mesa De Luz/cajonera Industrial - Hierro Y Madera   \n",
       "681903   Seidio Superficie Caso Con El Metal Pata De Ca...   \n",
       "\n",
       "                                  category  \n",
       "357743               HAIRDRESSING_SCISSORS  \n",
       "442017   DESKTOP_COMPUTER_COOLERS_AND_FANS  \n",
       "1097459                               PENS  \n",
       "547021                         NIGHTSTANDS  \n",
       "681903                          CELLPHONES  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train.sample(20000)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85ad84cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>911924</th>\n",
       "      <td>Torno Lavore Mhas 200 De 1,5mts X 400  Mmg Maq...</td>\n",
       "      <td>LATHES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037907</th>\n",
       "      <td>Estereo Boss 628ua</td>\n",
       "      <td>CAR_STEREOS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946243</th>\n",
       "      <td>Super Sale! Mimo&amp;co. Mono Con Short Floreado. ...</td>\n",
       "      <td>JUMPSUITS_AND_OVERALLS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519679</th>\n",
       "      <td>Souvenir Regalo Reloj 20cm Con Foto</td>\n",
       "      <td>WALL_CLOCKS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618917</th>\n",
       "      <td>Paragolpes Tras Peugeot Partner 09-15 Concesio...</td>\n",
       "      <td>AUTOMOTIVE_FRONT_BUMPERS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     title  \\\n",
       "911924   Torno Lavore Mhas 200 De 1,5mts X 400  Mmg Maq...   \n",
       "1037907                                 Estereo Boss 628ua   \n",
       "946243   Super Sale! Mimo&co. Mono Con Short Floreado. ...   \n",
       "519679                 Souvenir Regalo Reloj 20cm Con Foto   \n",
       "618917   Paragolpes Tras Peugeot Partner 09-15 Concesio...   \n",
       "\n",
       "                         category  \n",
       "911924                     LATHES  \n",
       "1037907               CAR_STEREOS  \n",
       "946243     JUMPSUITS_AND_OVERALLS  \n",
       "519679                WALL_CLOCKS  \n",
       "618917   AUTOMOTIVE_FRONT_BUMPERS  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df_test.sample(20000)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "929a7863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 2)\n",
      "(20000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e1d327",
   "metadata": {},
   "source": [
    "## Construcción del Dataset\n",
    "\n",
    "El dataset se construye a partir del dataframe de Pandas que tiene dos columnas:\n",
    "- **title**\n",
    "- **category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba04cc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeLiChallengeDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        item = {\n",
    "            \"data\": self.df.iloc[item][\"title\"],\n",
    "            \"target\": self.df.iloc[item][\"category\"]\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            item = self.transform(item)\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37e833f",
   "metadata": {},
   "source": [
    "## Preprocesamiento de los datos\n",
    "\n",
    "El preprocesamiento de texto tiene dos propósitos:\n",
    "- Tokenizar los títulos (datos) de modo que se quiten los signos de puntuación y palabras cortas como preposiciones y conjunciones (stopwords), todas las palabras queden en minúsculas, se separen en listas de palabras, etc.\n",
    "- Transformar las categorías en etiquetas numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13d206bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawDataProcessor:\n",
    "    def __init__(self, dataset, ignore_header=True, vocab_size=50000):\n",
    "        self.filters = [lambda s: s.lower(),\n",
    "                        preprocessing.strip_tags,\n",
    "                        preprocessing.strip_punctuation,\n",
    "                        preprocessing.strip_multiple_whitespaces,\n",
    "                        preprocessing.strip_numeric,\n",
    "                        preprocessing.remove_stopwords,\n",
    "                        preprocessing.strip_short]\n",
    "        \n",
    "        # Esta clase encapsula el mapeo entre las palabras normalizadas y sus correspondientes indices \n",
    "        # https://radimrehurek.com/gensim/corpora/dictionary.html\n",
    "        self.dictionary = corpora.Dictionary(dataset[\"title\"].map(self._preprocess_string).tolist())\n",
    "        \n",
    "        # Filter the dictionary with extremos words\n",
    "        # https://tedboy.github.io/nlps/generated/generated/gensim.corpora.Dictionary.filter_extremes.html?highlight=filter_extrem\n",
    "        self.dictionary.filter_extremes(no_below=2, no_above=1, keep_n=vocab_size)\n",
    "        \n",
    "        # Asigna nuevos índices a todas las palabras\n",
    "        # https://tedboy.github.io/nlps/generated/generated/gensim.corpora.Dictionary.compactify.html\n",
    "        # https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.compactify\n",
    "        self.dictionary.compactify()\n",
    "        \n",
    "        # Se agregan tokens especiales\n",
    "        self.dictionary.patch_with_special_tokens({\"[PAD]\": 0,\n",
    "                                                   \"[UNK]\": 1})\n",
    "        \n",
    "        # Conversión de categorías a etiquetas\n",
    "        self.idx_to_target = sorted(dataset[\"category\"].unique())\n",
    "        self.target_to_idx = {t: i for i, t in enumerate(self.idx_to_target)}\n",
    "\n",
    "\n",
    "    def _preprocess_string(self, string):\n",
    "        # Procesamiento de los títulos mediante la aplicación de una lista de filtros\n",
    "        # Parámetro: str -> El título sin procesar\n",
    "        # Salida: list -> Lista de strings\n",
    "        # https://radimrehurek.com/gensim/parsing/preprocessing.html#gensim.parsing.preprocessing.preprocess_string\n",
    "        return preprocessing.preprocess_string(string, filters=self.filters)\n",
    "\n",
    "    def _sentence_to_indices(self, sentence):\n",
    "        # Convierte una lista de palabras en una lista de índices\n",
    "        # Parámetro: list -> Lista de palabras\n",
    "        # Salida: list -> Lista de enteros (índices) en el mismo orden que las palabras\n",
    "        # https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx\n",
    "        return self.dictionary.doc2idx(sentence, unknown_word_index=1)\n",
    "    \n",
    "    def encode_data(self, data):\n",
    "        # Convierte un string en una lista de índices\n",
    "        return self._sentence_to_indices(self._preprocess_string(data))\n",
    "    \n",
    "    def encode_target(self, target):\n",
    "        # Convierte las categorías a etiquetas\n",
    "        return self.target_to_idx[target]\n",
    "    \n",
    "    def __call__(self, item):\n",
    "        if isinstance(item[\"data\"], str):\n",
    "            data = self.encode_data(item[\"data\"])\n",
    "        else:\n",
    "            data = [self.encode_data(d) for d in item[\"data\"]]\n",
    "        \n",
    "        if isinstance(item[\"target\"], str):\n",
    "            target = self.encode_target(item[\"target\"])\n",
    "        else:\n",
    "            target = [self.encode_target(t) for t in item[\"target\"]]\n",
    "        \n",
    "        return {\n",
    "            \"data\": data,\n",
    "            \"target\": target\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68c66071",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadSequences:\n",
    "    def __init__(self, pad_value=0, max_length=None, min_length=1):\n",
    "        assert max_length is None or min_length <= max_length\n",
    "        self.pad_value = pad_value\n",
    "        self.max_length = max_length\n",
    "        self.min_length = min_length\n",
    "\n",
    "    def __call__(self, items):\n",
    "        data, target = list(zip(*[(item[\"data\"], item[\"target\"]) for item in items]))\n",
    "        seq_lengths = [len(d) for d in data]\n",
    "\n",
    "        if self.max_length:\n",
    "            max_length = self.max_length\n",
    "            seq_lengths = [min(self.max_length, l) for l in seq_lengths]\n",
    "        else:\n",
    "            max_length = max(self.min_length, max(seq_lengths))\n",
    "\n",
    "        data = [d[:l] + [self.pad_value] * (max_length - l)\n",
    "                for d, l in zip(data, seq_lengths)]\n",
    "            \n",
    "        return {\n",
    "            \"data\": torch.LongTensor(data),\n",
    "            \"target\": torch.LongTensor(target)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "124b5563",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processor = RawDataProcessor(df_train, vocab_size=5000)\n",
    "train_dataset = MeLiChallengeDataset(df_train, transform=train_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b2a22f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_processor = RawDataProcessor(df_test, vocab_size=5000)\n",
    "test_dataset = MeLiChallengeDataset(df_test, transform=test_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a804f035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El dataset de entrenamiento tiene 20000 elementos.\n",
      "Elemento #100:\n",
      "\tData: [90, 287, 396, 395, 394]\n",
      "\tTarget: 25\n"
     ]
    }
   ],
   "source": [
    "i = 100\n",
    "print(f\"El dataset de entrenamiento tiene {len(train_dataset)} elementos.\")\n",
    "print(f\"Elemento #{i}:\\n\\tData: {train_dataset[i]['data']}\\n\\tTarget: {train_dataset[i]['target']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b720625c",
   "metadata": {},
   "source": [
    "## Carga del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e91fd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "pad_sequences = PadSequences()\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=pad_sequences,\n",
    "                          drop_last=False)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True,\n",
    "                         collate_fn=pad_sequences,\n",
    "                         drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdcaedca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d87a5a3851d4f6eaa28e0befc9a2d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 iteraciones\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for data in tqdm(train_loader):\n",
    "    i += 1\n",
    "print(f'{i} iteraciones')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4f7f30",
   "metadata": {},
   "source": [
    "## Construcción del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d1ae4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeLiChallengeClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 pretrained_embeddings_path, \n",
    "                 dictionary,\n",
    "                 vector_size,\n",
    "                 freeze_embedings):\n",
    "        super().__init__()\n",
    "        embeddings_matrix = torch.randn(len(dictionary), vector_size)\n",
    "        embeddings_matrix[0] = torch.zeros(vector_size)\n",
    "        with gzip.open(pretrained_embeddings_path, encode='utf-8', \"rt\") as fh:\n",
    "#       with bz2.open(pretrained_embeddings_path, \"rt\") as fh:\n",
    "            for line in fh:\n",
    "                word, vector = line.strip().split(None, 1)\n",
    "                if word in dictionary.token2id:\n",
    "                    embeddings_matrix[dictionary.token2id[word]] = torch.FloatTensor([float(n) for n in vector.split()])\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings_matrix,\n",
    "                                                       freeze=freeze_embedings,\n",
    "                                                       padding_idx=0)\n",
    "        self.hidden1 = nn.Linear(vector_size, 128)\n",
    "        self.hidden2 = nn.Linear(128, 128)\n",
    "        self.output = nn.Linear(128, 632)\n",
    "        self.vector_size = vector_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274fe854",
   "metadata": {},
   "source": [
    "## Algoritmo de Optimización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65a15de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.69 s, sys: 53 ms, total: 1.74 s\n",
      "Wall time: 1.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = MeLiChallengeClassifier(ARCHIVO_DE_EMBEDDINGS, train_processor.dictionary, 50, True)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7772d8",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dfe282f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilizando cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MeLiChallengeClassifier(\n",
       "  (embeddings): Embedding(5002, 50, padding_idx=0)\n",
       "  (hidden1): Linear(in_features=50, out_features=128, bias=True)\n",
       "  (hidden2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (output): Linear(in_features=128, out_features=632, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Utilizando {device}')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9777a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for epoch in range(EPOCHS):  # Recorre el dataset multiples veces\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for data in train_loader:\n",
    "#         inputs = data['data'].to(device)\n",
    "#         labels = data['target'].to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)\n",
    "#         loss = loss_function(outputs, labels.squeeze().long())\n",
    "#         loss.backward()\n",
    "#         optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4b74ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train()\n",
    "# for data in train_loader:\n",
    "#     inputs = data['data'].to(device)\n",
    "#     target = data['target'].to(device)\n",
    "#     optimizer.zero_grad()\n",
    "#     output = model(inputs)\n",
    "#     loss = loss_function(output, target)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "208c28aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, data in enumerate(dataloader):\n",
    "        X, y = data['data'].to(device), data['target'].to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f378dee4",
   "metadata": {},
   "source": [
    "## Evaluación del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "745425ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            X, y = data['data'].to(device), data['target'].to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa966304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 6.448280  [    0/20000]\n",
      "Test Error: \n",
      " Accuracy: 0.1%, Avg loss: 6.448738 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 6.448425  [    0/20000]\n",
      "Test Error: \n",
      " Accuracy: 0.1%, Avg loss: 6.448745 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 6.448981  [    0/20000]\n",
      "Test Error: \n",
      " Accuracy: 0.1%, Avg loss: 6.448733 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 6.448917  [    0/20000]\n",
      "Test Error: \n",
      " Accuracy: 0.1%, Avg loss: 6.448730 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 6.448575  [    0/20000]\n",
      "Test Error: \n",
      " Accuracy: 0.1%, Avg loss: 6.448724 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for t in range(EPOCHS):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, loss_function, optimizer)\n",
    "    test(test_loader, model, loss_function)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82f873c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# for batch, data in enumerate(train_loader):\n",
    "#     i += 1\n",
    "#     print (f'Lote {i}.')\n",
    "#     if i%10 == 0:\n",
    "#         print(type(data['data']))\n",
    "#         print(type(data['target']))\n",
    "# print(f'{i} iteraciones.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e169eaaa",
   "metadata": {},
   "source": [
    "## Guardado de los parámetros del modelo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
