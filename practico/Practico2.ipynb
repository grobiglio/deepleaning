{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad92109e",
   "metadata": {},
   "source": [
    "# Pr谩ctico 2\n",
    "\n",
    "[Enunciado](https://github.com/DiploDatos/AprendizajeProfundo/blob/master/Practico.md) del trabajo pr谩ctico.\n",
    "\n",
    "**Implementaci贸n de red neuronal [Red Neuronal Recurrente Long Short Term Memory](https://en.wikipedia.org/wiki/Long_short-term_memory) (LSTM).**\n",
    "\n",
    "## Integrantes\n",
    "- Mauricio Caggia\n",
    "- Luciano Monforte\n",
    "- Gustavo Venchiarutti\n",
    "- Guillermo Robiglio\n",
    "\n",
    "La raz贸n por la que se escoigi贸 una Red Neuronal Recurrente obedece a que la misma tiene aplicaciones en el Prcesamiento del Lenguaje Natural."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d74bd8",
   "metadata": {},
   "source": [
    "## Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d08584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from gensim import corpora\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from practico1_modulo import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e78fb0",
   "metadata": {},
   "source": [
    "## Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fb3a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fd7676",
   "metadata": {},
   "source": [
    "## Carga de datos\n",
    "\n",
    "Carga de datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02c88543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2878208/1027201965.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(torch.load('./data/y_train.pt'), dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.load('./data/X_train.pt')\n",
    "# y_train = torch.load('./data/y_train.pt')\n",
    "y_train = torch.tensor(torch.load('./data/y_train.pt'), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c4ec4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000000, 17])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# La reducci贸n del dataset de entrenamiento es temporal\n",
    "# Cuando compruebe que funciona se eliminar谩 esta celda.\n",
    "X_train = X_train[:1000000]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2005ef17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# La reducci贸n del dataset de entrenamiento es temporal\n",
    "# Cuando compruebe que funciona se eliminar谩 esta celda.\n",
    "y_train = y_train[:1000000]\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599bcadc",
   "metadata": {},
   "source": [
    "Carga de datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6081c5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2878208/2269913238.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test = torch.tensor(torch.load('./data/y_test.pt'), dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "X_test = torch.load('./data/X_test.pt')\n",
    "# y_test = torch.load('./data/y_test.pt')\n",
    "y_test = torch.tensor(torch.load('./data/y_test.pt'), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc46469e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500000, 16])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# La reducci贸n del dataset de prueba es temporal.\n",
    "# Cuando compruebe que funciona se eliminar谩 esta celda.\n",
    "X_test = X_test[:500000]\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb67b68f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# La reducci贸n del dataset de prueba es temporal.\n",
    "# Cuando compruebe que funciona se eliminar谩 esta celda.\n",
    "y_test = y_test[:500000]\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd06aa0",
   "metadata": {},
   "source": [
    "## Embedding de t铆tulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80989d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding\n",
    "embeddings_matrix = torch.load('./data/embeddings_matrix.pt')\n",
    "embeddings = nn.Embedding.from_pretrained(embeddings_matrix,\n",
    "                                          padding_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e1d327",
   "metadata": {},
   "source": [
    "## Construcci贸n del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "230d33c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0aec179d8314c349efc2289fb6cd911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorrida exitosa de 10000 batches de entrenamiento.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d8c28c5a684d02bb67cf8b7454bf5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorrida exitosa de 5000 batches de prueba.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MeLiChallengeDataset(X_train, y_train)\n",
    "test_dataset = MeLiChallengeDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,\n",
    "                          drop_last=False)\n",
    "i = 0\n",
    "for data in tqdm(train_loader):\n",
    "    i += 1\n",
    "print(f'Recorrida exitosa de {i} batches de entrenamiento.')\n",
    "\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         shuffle=True,\n",
    "                         drop_last=False)\n",
    "i = 0\n",
    "for data in tqdm(test_loader):\n",
    "    i += 1\n",
    "print(f'Recorrida exitosa de {i} batches de prueba.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4f7f30",
   "metadata": {},
   "source": [
    "## Construcci贸n del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3bb04b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeLiChallengeLSTM(nn.Module):\n",
    "    def __init__(self, embeddings):\n",
    "        super(MeLiChallengeLSTM, self).__init__()\n",
    "        self.embeddings = embeddings\n",
    "        output_size = 1\n",
    "        embedding_size = 300\n",
    "        hidden_layer = 32\n",
    "        num_layers = 1\n",
    "        bias = True\n",
    "        dropout = 0\n",
    "        bidirectional = False\n",
    "        self.lstm_config = {'input_size': embedding_size,\n",
    "                            'hidden_size': hidden_layer,\n",
    "                            'num_layers': num_layers,\n",
    "                            'bias': bias,\n",
    "                            'batch_first': True,\n",
    "                            'dropout': dropout,\n",
    "                            'bidirectional': bidirectional}\n",
    "        \n",
    "        # Set our fully connected layer parameters\n",
    "        self.linear_config = {'in_features': hidden_layer,\n",
    "                              'out_features': output_size,\n",
    "                              'bias': bias}\n",
    "        \n",
    "        # Instanciate the layers\n",
    "        self.lstm = nn.LSTM(**self.lstm_config)\n",
    "        self.classification_layer = nn.Linear(**self.linear_config)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        emb = self.embeddings(inputs)\n",
    "        lstm_out, _ = self.lstm(emb)\n",
    "        lstm_out = lstm_out[:, -1, :].squeeze()\n",
    "        predictions = self.activation(self.classification_layer(lstm_out))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a636a42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MeLiChallengeLSTM(\n",
      "  (embeddings): Embedding(50002, 300, padding_idx=0)\n",
      "  (lstm): LSTM(300, 32, batch_first=True)\n",
      "  (classification_layer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (activation): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "modelo = MeLiChallengeLSTM(embeddings)\n",
    "print(modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274fe854",
   "metadata": {},
   "source": [
    "## Algoritmo de Optimizaci贸n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65a15de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(modelo.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dfe282f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilizando cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MeLiChallengeLSTM(\n",
       "  (embeddings): Embedding(50002, 300, padding_idx=0)\n",
       "  (lstm): LSTM(300, 32, batch_first=True)\n",
       "  (classification_layer): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (activation): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Utilizando {device}')\n",
    "modelo.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7772d8",
   "metadata": {},
   "source": [
    "## Entrenamiento y evaluaci贸n del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c27edad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    '''Entrenamiento de una red neuronal.\n",
    "    \n",
    "    Par谩metros:\n",
    "    -----------\n",
    "    - dataloader: Iterador (objeto) de Pytorch construido en base al dataset basado en la clase MeLiChallengeDataset.\n",
    "    - model: Modelo (objeto) basado en la clase MeLiChallengeClassifier.\n",
    "    - loss_fn: Funci贸n de costo.\n",
    "    - optimizer: Optimizador.\n",
    "    \n",
    "    Salidas:\n",
    "    --------\n",
    "    train_loss: Valor promedio de la funci贸n de costo minimizados de cada uno de los batches.\n",
    "    \n",
    "    '''\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    running_loss = []\n",
    "    for batch, data in enumerate(tqdm(dataloader)):\n",
    "        X, y = data['data'].to(device), data['target'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X) # Esto devuelve siempre un tensor de unos, por eso no funciona \n",
    "        loss = loss_fn(pred.squeeze(), y) # Aqu铆 se compara un tensor de unos con los valores verdaderos\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss.append(loss.item())\n",
    "        train_loss = sum(running_loss) / len(running_loss)\n",
    "        \n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2a8bceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    '''Evaluaci贸n de una red neuronal.\n",
    "    \n",
    "    Par谩metros:\n",
    "    -----------\n",
    "    - dataloader: Iterador (objeto) de Pytorch construido en base al dataset basado en la clase MeLiChallengeDataset.\n",
    "    - model: Modelo (objeto) basado en la clase MeLiChallengeClassifier.\n",
    "    - loss_fn: Funci贸n de costo.\n",
    "    \n",
    "    Salidas:\n",
    "    --------\n",
    "    - train_loss: Valor promedio de la funci贸n de costo minimizados de cada uno de los batches.\n",
    "    - avp: Precisi贸n.\n",
    "    '''\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    running_loss = []\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(dataloader):\n",
    "            X, y = data['data'].to(device), data['target'].to(device)\n",
    "            pred = model(X) # Esto devuelve siempre un tensor de unos, por eso no funciona \n",
    "            running_loss.append(loss_function(pred.squeeze(), y).item())\n",
    "            targets.extend(y.cpu().detach().numpy())\n",
    "            predictions.extend(pred.squeeze().cpu().round().detach().numpy())\n",
    "            \n",
    "        test_loss = sum(running_loss) / len(running_loss)\n",
    "        avp = balanced_accuracy_score(targets, predictions)\n",
    "                                    \n",
    "    return test_loss, avp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be87f5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18427dcae8f4b14b0d9d20f2af877ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Final train_loss -31227.0784\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f413bdb3c5824d6fb316ffbd6187a33b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Final test_loss -31219.0474\n",
      "\t Final test_avp 0.0015822784810126582\n",
      "Epoch 2\n",
      "-------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c322a465546d4b2e8719dd668208d3e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Final train_loss -31227.0784\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afff7c24134c4456805e0b0aa378a6b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Final test_loss -31219.0474\n",
      "\t Final test_avp 0.0015822784810126582\n",
      "Epoch 3\n",
      "-------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f50e0cb4a94ee8b6f9a6785677e705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Final train_loss -31227.0784\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa82a55396548e3b60a9a332a1ebbbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Final test_loss -31219.0474\n",
      "\t Final test_avp 0.0015822784810126582\n",
      "!Listo!\n"
     ]
    }
   ],
   "source": [
    "history = {\n",
    "    'train_loss': [],\n",
    "    'test_loss': [],\n",
    "    'test_avp': []\n",
    "}\n",
    "\n",
    "for t in range(EPOCHS):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loss = train(train_loader, modelo, loss_function, optimizer)\n",
    "    print(\"\\t Final train_loss\", train_loss)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    test_loss, avp = test(test_loader, modelo, loss_function)\n",
    "    print(\"\\t Final test_loss\", test_loss)\n",
    "    print(\"\\t Final test_avp\", avp)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_avp'].append(avp)\n",
    "print(\"!Listo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccefb1c9",
   "metadata": {},
   "source": [
    "## 驴Por qu茅 no funciona?\n",
    "\n",
    "No funciona porque el modelo devuelve siempre un tensor de unos. Evidentemente hay un problema con el modelo, pero no s茅 como resolverlo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
