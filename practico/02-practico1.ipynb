{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad92109e",
   "metadata": {},
   "source": [
    "# Práctico 1 - Parte 2\n",
    "\n",
    "[Enunciado](https://github.com/DiploDatos/AprendizajeProfundo/blob/master/Practico.md) del trabajo práctico.\n",
    "\n",
    "**Implementación de red neuronal [Perceptrón Multicapa](https://en.wikipedia.org/wiki/Multilayer_perceptron) (MLP).**\n",
    "\n",
    "## Integrantes\n",
    "- Mauricio Caggia\n",
    "- Luciano Monforte\n",
    "- Gustavo Venchiarutti\n",
    "- Guillermo Robiglio\n",
    "\n",
    "En esta segunda parte se cargan datos reducidos en la parte 1. Esto con el fin de optimizar memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d74bd8",
   "metadata": {},
   "source": [
    "## Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d08584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import bz2\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.parsing import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e78fb0",
   "metadata": {},
   "source": [
    "## Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fb3a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHIVO_SET_DE_ENTRENAMIENTO_1 = './data/training_set1.csv'\n",
    "ARCHIVO_SET_DE_ENTRENAMIENTO_2 = './data/training_set2.csv'\n",
    "ARCHIVO_SET_DE_ENTRENAMIENTO_3 = './data/training_set3.csv'\n",
    "ARCHIVO_SET_DE_ENTRENAMIENTO_4 = './data/training_set4.csv'\n",
    "ARCHIVO_SET_DE_PRUEBA = './data/test_set.csv'\n",
    "ARCHIVO_SET_DE_VALIDACION = './data/validation_set.csv'\n",
    "ARCHIVO_DE_EMBEDDINGS = './data/SBW-vectors-300-min5.txt.bz2'\n",
    "# ARCHIVO_DE_EMBEDDINGS = '../data/glove.6B.50d.txt.gz'\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fd7676",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93f9d6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.24 s, sys: 274 ms, total: 1.52 s\n",
      "Wall time: 1.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file_paths = [ARCHIVO_SET_DE_ENTRENAMIENTO_1,\n",
    "              ARCHIVO_SET_DE_ENTRENAMIENTO_2,\n",
    "              ARCHIVO_SET_DE_ENTRENAMIENTO_3,\n",
    "              ARCHIVO_SET_DE_ENTRENAMIENTO_4,\n",
    "              ARCHIVO_SET_DE_PRUEBA,\n",
    "              ARCHIVO_SET_DE_VALIDACION]\n",
    "i = 0\n",
    "df = pd.read_csv(file_paths[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56c08a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Casita Muñecas Barbies Pintadas</td>\n",
       "      <td>DOLLHOUSES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Neceser Cromado Holográfico</td>\n",
       "      <td>TOILETRY_BAGS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Funda Asiento A Medida D20 Chevrolet</td>\n",
       "      <td>CAR_SEAT_COVERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Embrague Ford Focus One 1.8 8v Td (90cv) Desde...</td>\n",
       "      <td>AUTOMOTIVE_CLUTCH_KITS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bateria Panasonic Dmwbcf10 Lumix Dmc-fx60n Dmc...</td>\n",
       "      <td>CAMERA_BATTERIES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title                category\n",
       "0                    Casita Muñecas Barbies Pintadas              DOLLHOUSES\n",
       "1                       Neceser Cromado Holográfico            TOILETRY_BAGS\n",
       "2               Funda Asiento A Medida D20 Chevrolet         CAR_SEAT_COVERS\n",
       "3  Embrague Ford Focus One 1.8 8v Td (90cv) Desde...  AUTOMOTIVE_CLUTCH_KITS\n",
       "4  Bateria Panasonic Dmwbcf10 Lumix Dmc-fx60n Dmc...        CAMERA_BATTERIES"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e1d327",
   "metadata": {},
   "source": [
    "## Construcción del Dataset\n",
    "\n",
    "El dataset se construye a partir del dataframe de Pandas que tiene dos columnas:\n",
    "- **title**\n",
    "- **category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba04cc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeLiChallengeDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        item = {\n",
    "            \"data\": self.df.iloc[item][\"title\"],\n",
    "            \"target\": self.df.iloc[item][\"category\"]\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            item = self.transform(item)\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37e833f",
   "metadata": {},
   "source": [
    "## Preprocesamiento de los datos\n",
    "\n",
    "El preprocesamiento de texto tiene dos propósitos:\n",
    "- Tokenizar los títulos (datos) de modo que se quiten los signos de puntuación y palabras cortas como preposiciones y conjunciones (stopwords), todas las palabras queden en minúsculas, se separen en listas de palabras, etc.\n",
    "- Transformar las categorías en etiquetas numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13d206bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawDataProcessor:\n",
    "    def __init__(self, dataset, ignore_header=True, vocab_size=50000):\n",
    "        self.filters = [lambda s: s.lower(),\n",
    "                        preprocessing.strip_tags,\n",
    "                        preprocessing.strip_punctuation,\n",
    "                        preprocessing.strip_multiple_whitespaces,\n",
    "                        preprocessing.strip_numeric,\n",
    "                        preprocessing.remove_stopwords,\n",
    "                        preprocessing.strip_short]\n",
    "        \n",
    "        # Esta clase encapsula el mapeo entre las palabras normalizadas y sus correspondientes indices \n",
    "        # https://radimrehurek.com/gensim/corpora/dictionary.html\n",
    "        self.dictionary = corpora.Dictionary(\n",
    "            dataset[\"title\"].map(self._preprocess_string).tolist()\n",
    "        )\n",
    "        \n",
    "        # Filter the dictionary with extremos words\n",
    "        # https://tedboy.github.io/nlps/generated/generated/gensim.corpora.Dictionary.filter_extremes.html?highlight=filter_extrem\n",
    "        self.dictionary.filter_extremes(no_below=2, no_above=1, keep_n=vocab_size)\n",
    "        \n",
    "        # Asigna nuevos índices a todas las palabras\n",
    "        # https://tedboy.github.io/nlps/generated/generated/gensim.corpora.Dictionary.compactify.html\n",
    "        # https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.compactify\n",
    "        self.dictionary.compactify()\n",
    "        \n",
    "        # Se agregan tokens especiales\n",
    "        self.dictionary.patch_with_special_tokens({\"[PAD]\": 0,\n",
    "                                                   \"[UNK]\": 1})\n",
    "        \n",
    "        # Conversión de categorías a etiquetas\n",
    "        self.idx_to_target = sorted(dataset[\"category\"].unique())\n",
    "        self.target_to_idx = {t: i for i, t in enumerate(self.idx_to_target)}\n",
    "\n",
    "\n",
    "    def _preprocess_string(self, string):\n",
    "        # Procesamiento de los títulos mediante la aplicación de una lista de filtros\n",
    "        # Parámetro: str -> El título sin procesar\n",
    "        # Salida: list -> Lista de strings\n",
    "        # https://radimrehurek.com/gensim/parsing/preprocessing.html#gensim.parsing.preprocessing.preprocess_string\n",
    "        return preprocessing.preprocess_string(string, filters=self.filters)\n",
    "\n",
    "    def _sentence_to_indices(self, sentence):\n",
    "        # Convierte una lista de palabras en una lista de índices\n",
    "        # Parámetro: list -> Lista de palabras\n",
    "        # Salida: list -> Lista de enteros (índices) en el mismo orden que las palabras\n",
    "        # https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx\n",
    "        return self.dictionary.doc2idx(sentence, unknown_word_index=1)\n",
    "    \n",
    "    def encode_data(self, data):\n",
    "        # Convierte un string en una lista de índices\n",
    "        return self._sentence_to_indices(self._preprocess_string(data))\n",
    "    \n",
    "    def encode_target(self, target):\n",
    "        # Convierte las categorías a etiquetas\n",
    "        return self.target_to_idx[target]\n",
    "    \n",
    "    def __call__(self, item):\n",
    "        if isinstance(item[\"data\"], str):\n",
    "            data = self.encode_data(item[\"data\"])\n",
    "        else:\n",
    "            data = [self.encode_data(d) for d in item[\"data\"]]\n",
    "        \n",
    "        if isinstance(item[\"target\"], str):\n",
    "            target = self.encode_target(item[\"target\"])\n",
    "        else:\n",
    "            target = [self.encode_target(t) for t in item[\"target\"]]\n",
    "        \n",
    "        return {\n",
    "            \"data\": data,\n",
    "            \"target\": target\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68c66071",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadSequences:\n",
    "    def __init__(self, pad_value=0, max_length=None, min_length=1):\n",
    "        assert max_length is None or min_length <= max_length\n",
    "        self.pad_value = pad_value\n",
    "        self.max_length = max_length\n",
    "        self.min_length = min_length\n",
    "\n",
    "    def __call__(self, items):\n",
    "        data, target = list(zip(*[(item[\"data\"], item[\"target\"]) for item in items]))\n",
    "        seq_lengths = [len(d) for d in data]\n",
    "\n",
    "        if self.max_length:\n",
    "            max_length = self.max_length\n",
    "            seq_lengths = [min(self.max_length, l) for l in seq_lengths]\n",
    "        else:\n",
    "            max_length = max(self.min_length, max(seq_lengths))\n",
    "\n",
    "        data = [d[:l] + [self.pad_value] * (max_length - l)\n",
    "                for d, l in zip(data, seq_lengths)]\n",
    "            \n",
    "        return {\n",
    "            \"data\": torch.LongTensor(data),\n",
    "            \"target\": torch.FloatTensor(target)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "124b5563",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = RawDataProcessor(df)\n",
    "dataset = MeLiChallengeDataset(df, transform=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a804f035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El dataset tiene 1200001 elementos.\n",
      "Elemento #1000:\n",
      "\tData: [3024, 164]\n",
      "\tTarget: 459\n"
     ]
    }
   ],
   "source": [
    "i = 1000\n",
    "print(f\"El dataset tiene {len(dataset)} elementos.\")\n",
    "print(f\"Elemento #{i}:\\n\\tData: {dataset[i]['data']}\\n\\tTarget: {dataset[i]['target']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b720625c",
   "metadata": {},
   "source": [
    "## Carga del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e91fd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sequences = PadSequences()\n",
    "train_loader = DataLoader(dataset,\n",
    "                          batch_size=100,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=pad_sequences,\n",
    "                          drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4f7f30",
   "metadata": {},
   "source": [
    "## Construcción del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27fdf2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeLiChallengeClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 pretrained_embeddings_path, \n",
    "                 dictionary,\n",
    "                 vector_size,\n",
    "                 freeze_embedings):\n",
    "        super().__init__()\n",
    "        embeddings_matrix = torch.randn(len(dictionary), vector_size)\n",
    "        embeddings_matrix[0] = torch.zeros(vector_size)\n",
    "        with gzip.open(pretrained_embeddings_path, \"rt\") as fh:\n",
    "            for line in fh:\n",
    "                word, vector = line.strip().split(None, 1)\n",
    "                if word in dictionary.token2id:\n",
    "                    embeddings_matrix[dictionary.token2id[word]] =\\\n",
    "                        torch.FloatTensor([float(n) for n in vector.split()])\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings_matrix,\n",
    "                                                       freeze=freeze_embedings,\n",
    "                                                       padding_idx=0)\n",
    "        self.hidden1 = nn.Linear(vector_size, 128)\n",
    "        self.hidden2 = nn.Linear(128, 128)\n",
    "        self.output = nn.Linear(128, 632)\n",
    "        self.vector_size = vector_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d1ae4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeLiChallengeClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 pretrained_embeddings_path, \n",
    "                 dictionary,\n",
    "                 vector_size,\n",
    "                 freeze_embedings):\n",
    "        super().__init__()\n",
    "        embeddings_matrix = torch.randn(len(dictionary), vector_size)\n",
    "        embeddings_matrix[0] = torch.zeros(vector_size)\n",
    "        with bz2.open(pretrained_embeddings_path, \"rt\") as fh:\n",
    "            for line in fh:\n",
    "                word, vector = line.strip().split(None, 1)\n",
    "                if word in dictionary.token2id:\n",
    "                    embeddings_matrix[dictionary.token2id[word]] = torch.FloatTensor([float(n) for n in vector.split()])\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings_matrix,\n",
    "                                                       freeze=freeze_embedings,\n",
    "                                                       padding_idx=0)\n",
    "        self.hidden1 = nn.Linear(vector_size, 128)\n",
    "        self.hidden2 = nn.Linear(128, 128)\n",
    "        self.output = nn.Linear(128, 632)\n",
    "        self.vector_size = vector_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274fe854",
   "metadata": {},
   "source": [
    "## Algoritmo de Optimización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65a15de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MeLiChallengeClassifier(ARCHIVO_DE_EMBEDDINGS, processor.dictionary, 300, True)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7772d8",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4dfe282f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9777a43",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1) to match target batch_size (0).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:9\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.9/site-packages/torch/nn/modules/loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.9/site-packages/torch/nn/functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3013\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3014\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (1) to match target batch_size (0)."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(EPOCHS):  # Recorre el dataset multiples veces\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for data in train_loader:\n",
    "        inputs = data['data'].to(device)\n",
    "        labels = data['target'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels.squeeze().long())\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f378dee4",
   "metadata": {},
   "source": [
    "## Evaluación del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e169eaaa",
   "metadata": {},
   "source": [
    "## Guardado de los parámetros del modelo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
