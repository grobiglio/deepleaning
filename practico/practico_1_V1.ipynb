{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad92109e",
   "metadata": {},
   "source": [
    "# Pr√°ctico 1\n",
    "\n",
    "[Enunciado](https://github.com/DiploDatos/AprendizajeProfundo/blob/master/Practico.md) del trabajo pr√°ctico.\n",
    "\n",
    "**Implementaci√≥n de red neuronal [Perceptr√≥n Multicapa](https://en.wikipedia.org/wiki/Multilayer_perceptron) (MLP).**\n",
    "\n",
    "## Integrantes\n",
    "- Mauricio Caggia\n",
    "- Luciano Monforte\n",
    "- Gustavo Venchiarutti\n",
    "- Guillermo Robiglio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d74bd8",
   "metadata": {},
   "source": [
    "## Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d08584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "import functools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "from gensim.parsing import preprocessing\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e78fb0",
   "metadata": {},
   "source": [
    "## Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d5a343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHIVO_SET_DE_ENTRENAMIENTO = './data/meli-challenge-2019/spanish.train.jsonl.gz'\n",
    "ARCHIVO_SET_DE_PRUEBA = './data/meli-challenge-2019/spanish.test.jsonl.gz'\n",
    "ARCHIVO_SET_DE_VALIDACION = './data/meli-challenge-2019/spanish.validation.jsonl.gz'\n",
    "ARCHIVO_TOKENS = './data/meli-challenge-2019/spanish_token_to_index.json.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fd7676",
   "metadata": {},
   "source": [
    "## Carga de datos\n",
    "\n",
    "Esta carga de datos se realiza con el fin de explorar los mismos. Otra carga de datos tendr√° lugar al comento de construir el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc2d537d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingresar opci√≥n para carga de archivo (0 a 2): 2\n",
      "CPU times: user 22.6 s, sys: 5.73 s, total: 28.3 s\n",
      "Wall time: 33.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file_paths = [ARCHIVO_SET_DE_ENTRENAMIENTO, # Ingresar opci√≥n 0 üëÅ ‚ö† Tarda m√°s de 2 minutos en cargar y puede que haya un desbordamiento de RAM o muera el kernel‚ö†\n",
    "              ARCHIVO_SET_DE_PRUEBA,  # Ingresar opci√≥n 1\n",
    "              ARCHIVO_SET_DE_VALIDACION] # Ingresar opci√≥n 2 ‚ö† Tarda m√°s de 30 segundos en cargar\n",
    "i = int(input('Ingresar opci√≥n para carga de archivo (0 a 2): '))\n",
    "df = pd.read_json(path_or_buf=file_paths[i], lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5d8b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = pd.read_json(path_or_buf=ARCHIVO_TOKENS, lines=True).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1304190e",
   "metadata": {},
   "source": [
    "## An√°lisis y visualizaci√≥n de los datos\n",
    "\n",
    "El **set de entrenamiento** original tiene 4895280 registros con valores no nulos y 10 columnas. Las columnas de dicho dataset son:\n",
    "- **language**: El idioma del dataset (espa√±or o portugu√©s). En el trabajo pr√°ctico utilizaremos solamente el dataset es espa√±ol.\n",
    "- **label_quality**: Calidad de la etiqueta (confiable o no confiable). Se dispone de 4508043 registros no confiables y 387237 registros confiables.\n",
    "- **title**: El t√≠tulo que se asign√≥ al producto. **Esta informaci√≥n es la que se utilizar√° para armar el dataser de entrenamiento.**\n",
    "- **category**: La categor√≠a que se asign√≥ al producto. **Este es el target**.\n",
    "- **split**: El tipo de dataset. _train_ para el set de entrenamiento.\n",
    "- **tokenized_title**: El t√≠tulo tokenizado. Esto significa que los datos fueron preprocesados.\n",
    "- **data**: El n√∫mero asignado a cada palabra del t√≠tulo tokenizado.\n",
    "- **target**: El n√∫mero que corresponde a cada categor√≠a.\n",
    "- **n_labels**: Cantidad de etiquetas num√©ricas correspondientes a las distintas categor√≠as. 632 etiquetas (0 a 631) para el caso del set de entrenamiento.\n",
    "- **size**: La cantidad de registros. 4895280 registros para el caso del set de entrenamiento.\n",
    "\n",
    "El **set de prueba** original tiene 63680 registros con valores no nulos y 10 columnas. Las columnas de dicho dataset son:\n",
    "- **language**: El idioma del dataset (espa√±or o portugu√©s). En el trabajo pr√°ctico utilizaremos solamente el dataset es espa√±ol.\n",
    "- **label_quality**: Calidad de la etiqueta (confiable o no confiable). Todas las etiquetas de este dataset son confiables.\n",
    "- **title**: El t√≠tulo que se asign√≥ al producto.\n",
    "- **category**: La categor√≠a que se asign√≥ al producto.\n",
    "- **split**: El tipo de dataset. _test_ para el set de prueba.\n",
    "- **tokenized_title**: El t√≠tulo tokenizado. Esto significa que los datos fueron preprocesados.\n",
    "- **data**: El n√∫mero asignado a cada palabra del t√≠tulo tokenizado.\n",
    "- **target**: El n√∫mero que corresponde a cada categor√≠a.\n",
    "- **n_labels**: Cantidad de etiquetas num√©ricas correspondientes a las distintas categor√≠as. 632 etiquetas (0 a 631) para el caso del set de prueba.\n",
    "- **size**: La cantidad de registros. 63680 registros para el caso del set de prueba.\n",
    "\n",
    "El **set de validaci√≥n** original tiene 1223820 registros con valores no nulos y 10 columnas. Las columnas de dicho dataset son:\n",
    "- **language**: El idioma del dataset (espa√±or o portugu√©s). En el trabajo pr√°ctico utilizaremos solamente el dataset es espa√±ol.\n",
    "- **label_quality**: Calidad de la etiqueta (confiable o no confiable). Se dispone de 1127189 registros no confiables y 96631 registros confiables.\n",
    "- **title**: El t√≠tulo que se asign√≥ al producto.\n",
    "- **category**: La categor√≠a que se asign√≥ al producto.\n",
    "- **split**: El tipo de dataset. _validation_ para el set de prueba.\n",
    "- **tokenized_title**: El t√≠tulo tokenizado. Esto significa que los datos fueron preprocesados.\n",
    "- **data**: El n√∫mero asignado a cada palabra del t√≠tulo tokenizado.\n",
    "- **target**: El n√∫mero que corresponde a cada categor√≠a.\n",
    "- **n_labels**: Cantidad de etiquetas num√©ricas correspondientes a las distintas categor√≠as. 632 etiquetas (0 a 631) para el caso del set de validaci√≥n.\n",
    "- **size**: La cantidad de registros. 1223820 registros para el caso del set de validaci√≥n.\n",
    "\n",
    "El archivo **spanish_token_to_index** tiene las 50002 correspondencias que existen entre las palabras tokenizadas del t√≠tulo y las etiquetas num√©ricas bajo la columna data en los sets de entrenamiento, prueba y validaci√≥n. No se utilizar√° este tokenizador, en lugar de ello se utilizar√° ...\n",
    "\n",
    "**En este trabajo pr√°ctico se utiliza:**\n",
    "- El **set de entrenamiento** para entrenar el modelo\n",
    "- El **set de validaci√≥n** para evaluar el modelo y ajustar hiperpar√°metros\n",
    "- El **set de prueba** para mostrar el mejor modelo obtenido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56c08a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>label_quality</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>split</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>data</th>\n",
       "      <th>target</th>\n",
       "      <th>n_labels</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spanish</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>Metal Biela Dw10 Hdi 2.0</td>\n",
       "      <td>ENGINE_BEARINGS</td>\n",
       "      <td>validation</td>\n",
       "      <td>[metal, biela, hdi]</td>\n",
       "      <td>[457, 1480, 3450]</td>\n",
       "      <td>88</td>\n",
       "      <td>632</td>\n",
       "      <td>1223820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spanish</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>Repuestos Martillo Rotoprcutor Bosch Gshsce Po...</td>\n",
       "      <td>ELECTRIC_DEMOLITION_HAMMERS</td>\n",
       "      <td>validation</td>\n",
       "      <td>[repuestos, martillo, rotoprcutor, bosch, gshs...</td>\n",
       "      <td>[3119, 892, 1, 767, 1, 9337]</td>\n",
       "      <td>174</td>\n",
       "      <td>632</td>\n",
       "      <td>1223820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spanish</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>Pesca Ca√±a Pejerrey Colony Brava 3m Fibra De V...</td>\n",
       "      <td>FISHING_RODS</td>\n",
       "      <td>validation</td>\n",
       "      <td>[pesca, ca√±a, pejerrey, colony, brava, fibra, ...</td>\n",
       "      <td>[700, 990, 2057, 3990, 3670, 1737, 1153, 6568]</td>\n",
       "      <td>313</td>\n",
       "      <td>632</td>\n",
       "      <td>1223820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spanish</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>Porcelanato Abitare Be 20x120 Cm. Ceramica Por...</td>\n",
       "      <td>PORCELAIN_TILES</td>\n",
       "      <td>validation</td>\n",
       "      <td>[porcelanato, abitare, ceramica, portinari]</td>\n",
       "      <td>[2722, 4404, 1406, 4405]</td>\n",
       "      <td>427</td>\n",
       "      <td>632</td>\n",
       "      <td>1223820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spanish</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>Reconstruction Semi Di Lino Alfaparf Shampoo 1...</td>\n",
       "      <td>HAIR_SHAMPOOS_AND_CONDITIONERS</td>\n",
       "      <td>validation</td>\n",
       "      <td>[reconstruction, semi, lino, alfaparf, shampoo]</td>\n",
       "      <td>[1, 3365, 7502, 10919, 849]</td>\n",
       "      <td>194</td>\n",
       "      <td>632</td>\n",
       "      <td>1223820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language label_quality                                              title  \\\n",
       "0  spanish    unreliable                           Metal Biela Dw10 Hdi 2.0   \n",
       "1  spanish    unreliable  Repuestos Martillo Rotoprcutor Bosch Gshsce Po...   \n",
       "2  spanish    unreliable  Pesca Ca√±a Pejerrey Colony Brava 3m Fibra De V...   \n",
       "3  spanish    unreliable  Porcelanato Abitare Be 20x120 Cm. Ceramica Por...   \n",
       "4  spanish    unreliable  Reconstruction Semi Di Lino Alfaparf Shampoo 1...   \n",
       "\n",
       "                         category       split  \\\n",
       "0                 ENGINE_BEARINGS  validation   \n",
       "1     ELECTRIC_DEMOLITION_HAMMERS  validation   \n",
       "2                    FISHING_RODS  validation   \n",
       "3                 PORCELAIN_TILES  validation   \n",
       "4  HAIR_SHAMPOOS_AND_CONDITIONERS  validation   \n",
       "\n",
       "                                     tokenized_title  \\\n",
       "0                                [metal, biela, hdi]   \n",
       "1  [repuestos, martillo, rotoprcutor, bosch, gshs...   \n",
       "2  [pesca, ca√±a, pejerrey, colony, brava, fibra, ...   \n",
       "3        [porcelanato, abitare, ceramica, portinari]   \n",
       "4    [reconstruction, semi, lino, alfaparf, shampoo]   \n",
       "\n",
       "                                             data  target  n_labels     size  \n",
       "0                               [457, 1480, 3450]      88       632  1223820  \n",
       "1                    [3119, 892, 1, 767, 1, 9337]     174       632  1223820  \n",
       "2  [700, 990, 2057, 3990, 3670, 1737, 1153, 6568]     313       632  1223820  \n",
       "3                        [2722, 4404, 1406, 4405]     427       632  1223820  \n",
       "4                     [1, 3365, 7502, 10919, 849]     194       632  1223820  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d67367",
   "metadata": {},
   "source": [
    "### Tokens y sus etiquetas\n",
    "\n",
    "Las siguientes 3 celdas de c√≥digo demuestran que la relaci√≥n entre los datos bajo las columnas `tokenized_title` y `data` est√° dada en el archivo `spanish_token_to_index` que vincula cada palabra a un √≠ndice num√©rico entero. De todos modos, esto es a modo informativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f7cdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 # Un √≠ndice cualquiera para extraer datos\n",
    "items = df.at[i, 'tokenized_title']\n",
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477604c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar la salida de esta celda con la de la siguiente\n",
    "df.at[i, 'data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bec401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nro_items = []\n",
    "for item in items:\n",
    "    id_item = tokens.loc[item][0]\n",
    "    nro_items.append(id_item)\n",
    "nro_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7a0f27",
   "metadata": {},
   "source": [
    "Para el presente trabajo pr√°ctico solamente interesan las columnas **title** y **category**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e2a0a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Metal Biela Dw10 Hdi 2.0</td>\n",
       "      <td>ENGINE_BEARINGS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Repuestos Martillo Rotoprcutor Bosch Gshsce Po...</td>\n",
       "      <td>ELECTRIC_DEMOLITION_HAMMERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pesca Ca√±a Pejerrey Colony Brava 3m Fibra De V...</td>\n",
       "      <td>FISHING_RODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Porcelanato Abitare Be 20x120 Cm. Ceramica Por...</td>\n",
       "      <td>PORCELAIN_TILES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reconstruction Semi Di Lino Alfaparf Shampoo 1...</td>\n",
       "      <td>HAIR_SHAMPOOS_AND_CONDITIONERS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                           Metal Biela Dw10 Hdi 2.0   \n",
       "1  Repuestos Martillo Rotoprcutor Bosch Gshsce Po...   \n",
       "2  Pesca Ca√±a Pejerrey Colony Brava 3m Fibra De V...   \n",
       "3  Porcelanato Abitare Be 20x120 Cm. Ceramica Por...   \n",
       "4  Reconstruction Semi Di Lino Alfaparf Shampoo 1...   \n",
       "\n",
       "                         category  \n",
       "0                 ENGINE_BEARINGS  \n",
       "1     ELECTRIC_DEMOLITION_HAMMERS  \n",
       "2                    FISHING_RODS  \n",
       "3                 PORCELAIN_TILES  \n",
       "4  HAIR_SHAMPOOS_AND_CONDITIONERS  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['language', 'label_quality', 'split', 'tokenized_title', 'data', 'target', 'n_labels', 'size'],\n",
    "        inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b671769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En caso de quererlo, se reduce la muestra a n items\n",
    "n = 1000000\n",
    "df = df.sample(n, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b0f8da",
   "metadata": {},
   "source": [
    "### Etiquetar el target.\n",
    "\n",
    "A cada una de las 632 categor√≠as del target se le asigna un valor entero entre 0 y 631. La relaci√≥n entre la categor√≠a u su etiqueta queda almacenada en un dataframe de Pandas llamado `df_categories`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e079aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df.category.unique()\n",
    "sorted_categories = np.sort(categories)\n",
    "df_categories = pd.DataFrame(sorted_categories, columns=['categories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29920e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categories['cat_tag'] = pd.DataFrame(list(range(df_categories.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c279da02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_tag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categories</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3D_PRINTERS</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACCORDIONS</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACOUSTIC_GUITARS</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACTION_FIGURES</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADHESIVE_TAPES</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  cat_tag\n",
       "categories               \n",
       "3D_PRINTERS             0\n",
       "ACCORDIONS              1\n",
       "ACOUSTIC_GUITARS        2\n",
       "ACTION_FIGURES          3\n",
       "ADHESIVE_TAPES          4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_categories.set_index('categories', inplace=True)\n",
    "df_categories.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e1d327",
   "metadata": {},
   "source": [
    "## Construcci√≥n del Dataset\n",
    "\n",
    "El dataset se construye a partir de un dataframe de Pandas. El mismo debe tener al menos dos columnas:\n",
    "- **title**\n",
    "- **category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba04cc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El dataset tiene 1000000 elementos.\n",
      "Elemento #1000:\n",
      "\tData: Gorra Trucker Camionero Insecto De Steampunk Gorro De Camio\n",
      "\tTarget: HATS_AND_CAPS\n"
     ]
    }
   ],
   "source": [
    "class MeLiChallengeDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        item = {\n",
    "            \"data\": self.df.iloc[item][\"title\"],\n",
    "            \"target\": self.df.iloc[item][\"category\"]\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            item = self.transform(item)\n",
    "        \n",
    "        return item\n",
    "\n",
    "dataset = MeLiChallengeDataset(df)\n",
    "i = 1000\n",
    "print(f\"El dataset tiene {len(dataset)} elementos.\")\n",
    "print(f\"Elemento #{i}:\\n\\tData: {dataset[i]['data']}\\n\\tTarget: {dataset[i]['target']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37e833f",
   "metadata": {},
   "source": [
    "## Preprocesamiento de los datos\n",
    "\n",
    "El preprocesamiento de texto tiene dos prop√≥sitos:\n",
    "- Tokenizar los t√≠tulos (datos) de modo que se quiten los signos de puntuaci√≥n y palabras cortas como preposiciones y conjunciones (stopwords), todas las palabras queden en min√∫sculas, se separen en listas de palabras, etc.\n",
    "- Transformar las categor√≠as en etiquetas num√©ricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b47974aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: ['gorra', 'trucker', 'camionero', 'insecto', 'steampunk', 'gorro', 'camio']\n",
      "Target: 298\n"
     ]
    }
   ],
   "source": [
    "class TextPreprocess:\n",
    "    def __init__(self, df_categories, filters=None):\n",
    "        if filters:\n",
    "            self.filters = filters\n",
    "        else:\n",
    "            self.filters = [\n",
    "                lambda s: s.lower(),\n",
    "                preprocessing.strip_tags,\n",
    "                preprocessing.strip_punctuation,\n",
    "                preprocessing.strip_multiple_whitespaces,\n",
    "                preprocessing.strip_numeric,\n",
    "                preprocessing.remove_stopwords,\n",
    "                preprocessing.strip_short,\n",
    "            ]\n",
    "        self.df_categories = df_categories\n",
    "        \n",
    "    def _preprocess_string(self, string):\n",
    "        return preprocessing.preprocess_string(string, filters=self.filters)\n",
    "\n",
    "    def __call__(self, item):\n",
    "        if isinstance(item[\"data\"], str):\n",
    "            data = self._preprocess_string(item[\"data\"])\n",
    "        else:\n",
    "            data = [self._preprocess_string(d) for d in item[\"data\"]]\n",
    "        \n",
    "        category = item[\"target\"]\n",
    "        target = self.df_categories.at[category, 'cat_tag']\n",
    "        \n",
    "        return {\n",
    "            \"data\": data,\n",
    "            \"target\": target\n",
    "        }\n",
    "\n",
    "text_preprocess = TextPreprocess(df_categories)\n",
    "print(f\"Data: {text_preprocess(dataset[i])['data']}\\nTarget: {text_preprocess(dataset[i])['target']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77568602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MeLiChallengeDataset(IterableDataset):\n",
    "\n",
    "#     def __init__(self, path, transform=None):\n",
    "#         self.dataset_path = path\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         with gzip.open(self.dataset_path, \"rt\") as json_file:\n",
    "#             for line in json_file:\n",
    "#                 data = json.loads(line)\n",
    "#                 item = {\n",
    "#                     \"data\": data['title'],\n",
    "#                     \"target\": data['category']\n",
    "#                 }\n",
    "#                 if self.transform:\n",
    "#                     yield self.transform(item)\n",
    "#                 else:\n",
    "#                     yield item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f72cd3",
   "metadata": {},
   "source": [
    "## Vectorizaci√≥n de palabras\n",
    "Transforma las palabras en n√∫meros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "debcb9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[-0.58222002, -0.13591   , -0.11424   , -0.45646   , -0.46757001,\n",
       "         -0.71284997,  0.57985997,  0.28141001,  0.016912  ,  0.39019999,\n",
       "         -0.28990999,  0.71055001, -0.26488   , -0.27603999, -0.14974   ,\n",
       "         -0.36019   ,  0.11853   , -0.29010999,  0.67057002,  0.25435001,\n",
       "         -0.12891001,  0.11243   , -0.29857999,  0.66944999, -0.034637  ,\n",
       "          1.07770002,  0.43039   , -0.15572999, -0.49928999,  0.56309003,\n",
       "         -1.62989998, -0.58959001,  1.12020004, -0.18444   , -0.20709001,\n",
       "         -0.15952   ,  0.021452  , -0.066128  , -0.046139  , -0.75445998,\n",
       "          0.57239997, -0.28136   ,  0.094567  ,  0.23831999,  0.39313999,\n",
       "         -0.16561   ,  0.70946997, -0.51532   ,  0.05054   ,  1.2148    ],\n",
       "        [-0.88726997, -0.36691999, -0.14989001, -0.83456999, -0.13584   ,\n",
       "          0.33687001, -0.63673002,  0.41089001,  0.96688998, -0.10296   ,\n",
       "          0.21755999,  0.016873  , -0.2353    ,  0.47911999,  0.041009  ,\n",
       "         -0.79082   ,  0.14113   ,  0.63757002, -0.068506  , -0.40558001,\n",
       "         -0.41646999,  0.56270999, -0.57233   ,  0.64854997, -0.12621   ,\n",
       "         -0.85058999,  0.020916  ,  0.30267   ,  0.69047999,  0.33309001,\n",
       "         -0.34075001,  0.10273   , -0.046185  ,  1.43309999,  0.38903001,\n",
       "          0.060383  ,  0.10079   , -0.54624999, -0.21153   ,  0.61887997,\n",
       "          0.086107  ,  0.69148999,  0.055235  ,  0.27711001, -0.027312  ,\n",
       "         -1.0151    , -0.43008   , -0.055043  ,  0.26925001, -0.15879001],\n",
       "        [ 0.41669456,  0.02085394, -0.08382513,  0.16302977, -0.57731305,\n",
       "          1.10875631, -0.72508162, -0.75487657, -0.2102024 ,  0.74929304,\n",
       "          0.29547131,  1.0374621 , -0.13307943, -0.18735662, -0.83288769,\n",
       "          0.13543852, -1.10068032,  0.46525793,  1.04262278,  0.7796873 ,\n",
       "          0.01114804, -0.94039045, -2.11101896, -0.81160047, -1.01881749,\n",
       "         -2.04997737, -0.98062144, -0.0877128 ,  1.06673162, -0.15383446,\n",
       "         -0.95215748, -0.21197929,  0.15442404, -0.80171823, -0.77630914,\n",
       "          0.26467032, -0.95349426,  1.49168581, -0.76878293, -0.74942375,\n",
       "         -0.44401571, -1.71120734, -1.03344516,  0.27360153, -1.17365296,\n",
       "         -1.49529391, -0.37386662, -0.24373573,  1.24496308, -0.68094262],\n",
       "        [ 0.41669456,  0.02085394, -0.08382513,  0.16302977, -0.57731305,\n",
       "          1.10875631, -0.72508162, -0.75487657, -0.2102024 ,  0.74929304,\n",
       "          0.29547131,  1.0374621 , -0.13307943, -0.18735662, -0.83288769,\n",
       "          0.13543852, -1.10068032,  0.46525793,  1.04262278,  0.7796873 ,\n",
       "          0.01114804, -0.94039045, -2.11101896, -0.81160047, -1.01881749,\n",
       "         -2.04997737, -0.98062144, -0.0877128 ,  1.06673162, -0.15383446,\n",
       "         -0.95215748, -0.21197929,  0.15442404, -0.80171823, -0.77630914,\n",
       "          0.26467032, -0.95349426,  1.49168581, -0.76878293, -0.74942375,\n",
       "         -0.44401571, -1.71120734, -1.03344516,  0.27360153, -1.17365296,\n",
       "         -1.49529391, -0.37386662, -0.24373573,  1.24496308, -0.68094262],\n",
       "        [-0.79097003,  0.033223  , -1.39460003, -0.19238   , -0.34349999,\n",
       "         -0.021489  ,  0.11289   , -1.0869    , -0.092482  ,  0.90987998,\n",
       "         -0.14241   ,  0.07167   ,  0.092547  ,  0.88446999,  0.03318   ,\n",
       "         -0.29497999,  0.71626002,  0.32273999, -0.58174998, -0.23647   ,\n",
       "          0.49311   , -0.057614  , -0.40597001,  1.04260004,  0.28876999,\n",
       "          0.81150001, -0.90177   ,  0.025722  ,  0.37402001,  0.68427998,\n",
       "         -0.41622999, -1.04729998, -0.25745001, -0.86375999, -0.23037   ,\n",
       "          0.051651  , -0.18291999, -0.51722002, -1.03340006, -0.0075955 ,\n",
       "          0.17671999, -0.72595   , -0.0093165 ,  0.34654   ,  0.11644   ,\n",
       "          0.85784   ,  0.88343   , -0.096505  ,  0.17552   , -0.35076001],\n",
       "        [ 0.41669456,  0.02085394, -0.08382513,  0.16302977, -0.57731305,\n",
       "          1.10875631, -0.72508162, -0.75487657, -0.2102024 ,  0.74929304,\n",
       "          0.29547131,  1.0374621 , -0.13307943, -0.18735662, -0.83288769,\n",
       "          0.13543852, -1.10068032,  0.46525793,  1.04262278,  0.7796873 ,\n",
       "          0.01114804, -0.94039045, -2.11101896, -0.81160047, -1.01881749,\n",
       "         -2.04997737, -0.98062144, -0.0877128 ,  1.06673162, -0.15383446,\n",
       "         -0.95215748, -0.21197929,  0.15442404, -0.80171823, -0.77630914,\n",
       "          0.26467032, -0.95349426,  1.49168581, -0.76878293, -0.74942375,\n",
       "         -0.44401571, -1.71120734, -1.03344516,  0.27360153, -1.17365296,\n",
       "         -1.49529391, -0.37386662, -0.24373573,  1.24496308, -0.68094262],\n",
       "        [ 0.41669456,  0.02085394, -0.08382513,  0.16302977, -0.57731305,\n",
       "          1.10875631, -0.72508162, -0.75487657, -0.2102024 ,  0.74929304,\n",
       "          0.29547131,  1.0374621 , -0.13307943, -0.18735662, -0.83288769,\n",
       "          0.13543852, -1.10068032,  0.46525793,  1.04262278,  0.7796873 ,\n",
       "          0.01114804, -0.94039045, -2.11101896, -0.81160047, -1.01881749,\n",
       "         -2.04997737, -0.98062144, -0.0877128 ,  1.06673162, -0.15383446,\n",
       "         -0.95215748, -0.21197929,  0.15442404, -0.80171823, -0.77630914,\n",
       "          0.26467032, -0.95349426,  1.49168581, -0.76878293, -0.74942375,\n",
       "         -0.44401571, -1.71120734, -1.03344516,  0.27360153, -1.17365296,\n",
       "         -1.49529391, -0.37386662, -0.24373573,  1.24496308, -0.68094262]]),\n",
       " 'target': 298}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorizeText:\n",
    "    def __init__(self, glove_vectors_path):\n",
    "        self.glove_model = KeyedVectors.load_word2vec_format(\"../data/glove.6B.50d.txt\",\n",
    "                                                             binary=False,\n",
    "                                                             no_header=True)\n",
    "        self.unkown_vector = np.random.randn(self.glove_model.vector_size)  # Random vector for unknown words\n",
    "    \n",
    "    def _get_vector(self, word):\n",
    "        if word in self.glove_model:\n",
    "            return self.glove_model[word]\n",
    "        else:\n",
    "            return self.unkown_vector\n",
    "    \n",
    "    def _get_vectors(self, sentence):\n",
    "        return np.vstack([self._get_vector(word) for word in sentence])\n",
    "    \n",
    "    def __call__(self, item):\n",
    "        review = []\n",
    "        if isinstance(item[\"data\"][0], str):\n",
    "            review = self._get_vectors(item[\"data\"])\n",
    "        else:\n",
    "            review = [self._get_vectors(d) for d in item[\"data\"]]\n",
    "\n",
    "        return {\n",
    "            \"data\": review,\n",
    "            \"target\": item[\"target\"]\n",
    "        }\n",
    "vectorizer = VectorizeText(\"../data/glove.6B.50d.txt.gz\")\n",
    "vectorizer(text_preprocess(dataset[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ddaa276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([-8.48116842e-02, -5.51701767e-02, -2.84861511e-01, -1.18755843e-01,\n",
       "        -4.65166026e-01,  5.76793754e-01, -4.06329502e-01, -4.87729468e-01,\n",
       "         7.21576851e-03,  5.99184591e-01,  1.38160751e-01,  7.06991628e-01,\n",
       "        -1.34278675e-01,  4.83033598e-02, -4.86728823e-01, -1.29176559e-01,\n",
       "        -4.89543040e-01,  3.61604537e-01,  5.98686449e-01,  3.90149884e-01,\n",
       "        -1.09683350e-03, -4.49147973e-01, -1.38870798e+00, -1.26543126e-01,\n",
       "        -5.63906710e-01, -1.02304278e+00, -6.24707106e-01, -2.54556007e-02,\n",
       "         6.90305213e-01,  1.37874594e-01, -8.85072844e-01, -3.40296735e-01,\n",
       "         2.04894456e-01, -4.03138991e-01, -4.50523792e-01,  1.44456468e-01,\n",
       "        -5.53522146e-01,  6.91020748e-01, -6.23742968e-01, -4.48695784e-01,\n",
       "        -1.34405125e-01, -1.02294991e+00, -5.70470735e-01,  2.79482304e-01,\n",
       "        -6.01763408e-01, -9.00577949e-01, -4.75209274e-02, -2.34544418e-01,\n",
       "         7.82166046e-01, -2.88360070e-01]),\n",
       " 'target': 298}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WordVectorsAverage:\n",
    "    def __call__(self, item):\n",
    "        if item[\"data\"][0].ndim == 2:\n",
    "            data = np.vstack([np.mean(d, axis=0) for d in item[\"data\"]])\n",
    "        else:\n",
    "            data = np.mean(item[\"data\"], axis=0)\n",
    "        \n",
    "        return {\n",
    "            \"data\": data,\n",
    "            \"target\": item[\"target\"]\n",
    "        }\n",
    "\n",
    "vector_average = WordVectorsAverage()\n",
    "vector_average(vectorizer(text_preprocess(dataset[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0cbb0951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': tensor([-8.4812e-02, -5.5170e-02, -2.8486e-01, -1.1876e-01, -4.6517e-01,\n",
       "          5.7679e-01, -4.0633e-01, -4.8773e-01,  7.2158e-03,  5.9918e-01,\n",
       "          1.3816e-01,  7.0699e-01, -1.3428e-01,  4.8303e-02, -4.8673e-01,\n",
       "         -1.2918e-01, -4.8954e-01,  3.6160e-01,  5.9869e-01,  3.9015e-01,\n",
       "         -1.0968e-03, -4.4915e-01, -1.3887e+00, -1.2654e-01, -5.6391e-01,\n",
       "         -1.0230e+00, -6.2471e-01, -2.5456e-02,  6.9031e-01,  1.3787e-01,\n",
       "         -8.8507e-01, -3.4030e-01,  2.0489e-01, -4.0314e-01, -4.5052e-01,\n",
       "          1.4446e-01, -5.5352e-01,  6.9102e-01, -6.2374e-01, -4.4870e-01,\n",
       "         -1.3441e-01, -1.0229e+00, -5.7047e-01,  2.7948e-01, -6.0176e-01,\n",
       "         -9.0058e-01, -4.7521e-02, -2.3454e-01,  7.8217e-01, -2.8836e-01],\n",
       "        dtype=torch.float64),\n",
       " 'target': tensor(298)}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ToTensor:\n",
    "    def __call__(self, item):\n",
    "        \"\"\"\n",
    "        This espects a single array.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"data\": torch.from_numpy(item[\"data\"]),\n",
    "            \"target\": torch.tensor(item[\"target\"])\n",
    "        }\n",
    "\n",
    "to_tensor = ToTensor()\n",
    "to_tensor(vector_average(vectorizer(text_preprocess(dataset[i]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01245c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose(*functions):\n",
    "    return functools.reduce(lambda f, g: lambda x: g(f(x)), functions, lambda x: x)\n",
    "\n",
    "preprocess = TextPreprocess(df_categories)\n",
    "vectorizer = VectorizeText(ARCHIVO_TOKENS)\n",
    "vector_average = WordVectorsAverage()\n",
    "to_tensor = ToTensor()\n",
    "dataset = MeLiChallengeDataset(df,\n",
    "                               transform=compose(preprocess, vectorizer, vector_average, to_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cf4600",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, sample in enumerate(dataset):\n",
    "    print(sample[\"data\"])\n",
    "    print(sample[\"target\"])\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if idx == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b720625c",
   "metadata": {},
   "source": [
    "## Carga del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b12ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b85b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    print(i_batch, \n",
    "          sample_batched['data'].size(),\n",
    "          sample_batched['target'].size())\n",
    "\n",
    "    if i_batch == 2:\n",
    "        print(sample_batched[\"data\"])\n",
    "        print(sample_batched[\"target\"])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4f7f30",
   "metadata": {},
   "source": [
    "## Construcci√≥n del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fdf2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_layer1 = torch.nn.Linear(50, 512)\n",
    "        self.hidden_layer2 = torch.nn.Linear(512, 256)\n",
    "        self.output_layer = torch.nn.Linear(256, 632)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.hidden_layer1(x)  # Go through hidden layer 1\n",
    "        x = torch.nn.functional.relu(x)  # Activation Function layer 1\n",
    "        x = self.hidden_layer2(x) # Go through hidden layer 2\n",
    "        x = torch.nn.functional.relu(x)  # Activation Function layer 2\n",
    "        x = self.output_layer(x)  # Output Layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274fe854",
   "metadata": {},
   "source": [
    "## Algoritmo de Optimizaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a15de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7772d8",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6140ff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "model.train()  # Tell the model to set itself to \"train\" mode.\n",
    "for epoch in range(EPOCHS):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    # pbar = tqdm(dataloader)\n",
    "    for i, data in enumerate(dataloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        # inputs: tensor de Torch de dimensi√≥n [BATCH_SIZE, 3, 32, 32]\n",
    "        # labels: tensor de Torch de dimensi√≥n [BATCH_SIZE]\n",
    "        inputs = data[\"data\"]\n",
    "        labels = data[\"target\"]\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs) # Al modelo le ingreso como argumento un\n",
    "                                                          # tensor de Torch de dimensi√≥n [BATCH_SIZE, 3072]\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i > 0 and i % 50 == 0:    # print every 50 mini-batches\n",
    "            pbar.set_description(f\"[{epoch + 1}, {i}] loss: {running_loss / 50:.4g}\")\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f378dee4",
   "metadata": {},
   "source": [
    "## Evaluaci√≥n del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e169eaaa",
   "metadata": {},
   "source": [
    "## Guardado de los par√°metros del modelo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
