{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad92109e",
   "metadata": {},
   "source": [
    "# Práctico 1\n",
    "\n",
    "[Enunciado](https://github.com/DiploDatos/AprendizajeProfundo/blob/master/Practico.md) del trabajo práctico.\n",
    "\n",
    "**Implementación de red neuronal [Perceptrón Multicapa](https://en.wikipedia.org/wiki/Multilayer_perceptron) (MLP).**\n",
    "\n",
    "## Integrantes\n",
    "- Mauricio Caggia\n",
    "- Luciano Monforte\n",
    "- Gustavo Venchiarutti\n",
    "- Guillermo Robiglio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d74bd8",
   "metadata": {},
   "source": [
    "## Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d08584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.parsing import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e78fb0",
   "metadata": {},
   "source": [
    "## Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d5a343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHIVO_SET_DE_ENTRENAMIENTO = './data/meli-challenge-2019/spanish.train.jsonl.gz'\n",
    "ARCHIVO_SET_DE_PRUEBA = './data/meli-challenge-2019/spanish.test.jsonl.gz'\n",
    "ARCHIVO_SET_DE_VALIDACION = './data/meli-challenge-2019/spanish.validation.jsonl.gz'\n",
    "ARCHIVO_TOKENS = './data/meli-challenge-2019/spanish_token_to_index.json.gz'\n",
    "# ARCHIVO_DE_EMBEDDINGS = './data/SBW-vectors-300-min5.txt.bz2'\n",
    "ARCHIVO_DE_EMBEDDINGS = '../data/glove.6B.50d.txt.gz'\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fd7676",
   "metadata": {},
   "source": [
    "## Carga de datos\n",
    "\n",
    "Esta carga de datos se realiza con el fin de explorar los mismos. Otra carga de datos tendrá lugar al comento de construir el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc2d537d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.6 s, sys: 2.65 s, total: 21.3 s\n",
      "Wall time: 22.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file_paths = [ARCHIVO_SET_DE_ENTRENAMIENTO,\n",
    "              ARCHIVO_SET_DE_PRUEBA,\n",
    "              ARCHIVO_SET_DE_VALIDACION]\n",
    "i = 2 # Hasta que funcione\n",
    "df = pd.read_json(path_or_buf=file_paths[i], lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1304190e",
   "metadata": {},
   "source": [
    "## Análisis y visualización de los datos\n",
    "\n",
    "El **set de entrenamiento** original tiene 4895280 registros con valores no nulos y 10 columnas. Las columnas de dicho dataset son:\n",
    "- **language**: El idioma del dataset (españor o portugués). En el trabajo práctico utilizaremos solamente el dataset es español.\n",
    "- **label_quality**: Calidad de la etiqueta (confiable o no confiable). Se dispone de 4508043 registros no confiables y 387237 registros confiables.\n",
    "- **title**: El título que se asignó al producto. **Esta información es la que se utilizará para armar el dataser de entrenamiento.**\n",
    "- **category**: La categoría que se asignó al producto. **Este es el target**.\n",
    "- **split**: El tipo de dataset. _train_ para el set de entrenamiento.\n",
    "- **tokenized_title**: El título tokenizado. Esto significa que los datos fueron preprocesados.\n",
    "- **data**: El número asignado a cada palabra del título tokenizado.\n",
    "- **target**: El número que corresponde a cada categoría.\n",
    "- **n_labels**: Cantidad de etiquetas numéricas correspondientes a las distintas categorías. 632 etiquetas (0 a 631) para el caso del set de entrenamiento.\n",
    "- **size**: La cantidad de registros. 4895280 registros para el caso del set de entrenamiento.\n",
    "\n",
    "El **set de prueba** original tiene 63680 registros con valores no nulos y 10 columnas. Las columnas de dicho dataset son:\n",
    "- **language**: El idioma del dataset (españor o portugués). En el trabajo práctico utilizaremos solamente el dataset es español.\n",
    "- **label_quality**: Calidad de la etiqueta (confiable o no confiable). Todas las etiquetas de este dataset son confiables.\n",
    "- **title**: El título que se asignó al producto.\n",
    "- **category**: La categoría que se asignó al producto.\n",
    "- **split**: El tipo de dataset. _test_ para el set de prueba.\n",
    "- **tokenized_title**: El título tokenizado. Esto significa que los datos fueron preprocesados.\n",
    "- **data**: El número asignado a cada palabra del título tokenizado.\n",
    "- **target**: El número que corresponde a cada categoría.\n",
    "- **n_labels**: Cantidad de etiquetas numéricas correspondientes a las distintas categorías. 632 etiquetas (0 a 631) para el caso del set de prueba.\n",
    "- **size**: La cantidad de registros. 63680 registros para el caso del set de prueba.\n",
    "\n",
    "El **set de validación** original tiene 1223820 registros con valores no nulos y 10 columnas. Las columnas de dicho dataset son:\n",
    "- **language**: El idioma del dataset (españor o portugués). En el trabajo práctico utilizaremos solamente el dataset es español.\n",
    "- **label_quality**: Calidad de la etiqueta (confiable o no confiable). Se dispone de 1127189 registros no confiables y 96631 registros confiables.\n",
    "- **title**: El título que se asignó al producto.\n",
    "- **category**: La categoría que se asignó al producto.\n",
    "- **split**: El tipo de dataset. _validation_ para el set de prueba.\n",
    "- **tokenized_title**: El título tokenizado. Esto significa que los datos fueron preprocesados.\n",
    "- **data**: El número asignado a cada palabra del título tokenizado.\n",
    "- **target**: El número que corresponde a cada categoría.\n",
    "- **n_labels**: Cantidad de etiquetas numéricas correspondientes a las distintas categorías. 632 etiquetas (0 a 631) para el caso del set de validación.\n",
    "- **size**: La cantidad de registros. 1223820 registros para el caso del set de validación.\n",
    "\n",
    "El archivo **spanish_token_to_index** tiene las 50002 correspondencias que existen entre las palabras tokenizadas del título y las etiquetas numéricas bajo la columna data en los sets de entrenamiento, prueba y validación. No se utilizará este tokenizador, en lugar de ello se utilizará ...\n",
    "\n",
    "**En este trabajo práctico se utiliza:**\n",
    "- El **set de entrenamiento** para entrenar el modelo\n",
    "- El **set de validación** para evaluar el modelo y ajustar hiperparámetros\n",
    "- El **set de prueba** para mostrar el mejor modelo obtenido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56c08a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>label_quality</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>split</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>data</th>\n",
       "      <th>target</th>\n",
       "      <th>n_labels</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spanish</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>Metal Biela Dw10 Hdi 2.0</td>\n",
       "      <td>ENGINE_BEARINGS</td>\n",
       "      <td>validation</td>\n",
       "      <td>[metal, biela, hdi]</td>\n",
       "      <td>[457, 1480, 3450]</td>\n",
       "      <td>88</td>\n",
       "      <td>632</td>\n",
       "      <td>1223820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spanish</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>Repuestos Martillo Rotoprcutor Bosch Gshsce Po...</td>\n",
       "      <td>ELECTRIC_DEMOLITION_HAMMERS</td>\n",
       "      <td>validation</td>\n",
       "      <td>[repuestos, martillo, rotoprcutor, bosch, gshs...</td>\n",
       "      <td>[3119, 892, 1, 767, 1, 9337]</td>\n",
       "      <td>174</td>\n",
       "      <td>632</td>\n",
       "      <td>1223820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spanish</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>Pesca Caña Pejerrey Colony Brava 3m Fibra De V...</td>\n",
       "      <td>FISHING_RODS</td>\n",
       "      <td>validation</td>\n",
       "      <td>[pesca, caña, pejerrey, colony, brava, fibra, ...</td>\n",
       "      <td>[700, 990, 2057, 3990, 3670, 1737, 1153, 6568]</td>\n",
       "      <td>313</td>\n",
       "      <td>632</td>\n",
       "      <td>1223820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spanish</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>Porcelanato Abitare Be 20x120 Cm. Ceramica Por...</td>\n",
       "      <td>PORCELAIN_TILES</td>\n",
       "      <td>validation</td>\n",
       "      <td>[porcelanato, abitare, ceramica, portinari]</td>\n",
       "      <td>[2722, 4404, 1406, 4405]</td>\n",
       "      <td>427</td>\n",
       "      <td>632</td>\n",
       "      <td>1223820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spanish</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>Reconstruction Semi Di Lino Alfaparf Shampoo 1...</td>\n",
       "      <td>HAIR_SHAMPOOS_AND_CONDITIONERS</td>\n",
       "      <td>validation</td>\n",
       "      <td>[reconstruction, semi, lino, alfaparf, shampoo]</td>\n",
       "      <td>[1, 3365, 7502, 10919, 849]</td>\n",
       "      <td>194</td>\n",
       "      <td>632</td>\n",
       "      <td>1223820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language label_quality                                              title  \\\n",
       "0  spanish    unreliable                           Metal Biela Dw10 Hdi 2.0   \n",
       "1  spanish    unreliable  Repuestos Martillo Rotoprcutor Bosch Gshsce Po...   \n",
       "2  spanish    unreliable  Pesca Caña Pejerrey Colony Brava 3m Fibra De V...   \n",
       "3  spanish    unreliable  Porcelanato Abitare Be 20x120 Cm. Ceramica Por...   \n",
       "4  spanish    unreliable  Reconstruction Semi Di Lino Alfaparf Shampoo 1...   \n",
       "\n",
       "                         category       split  \\\n",
       "0                 ENGINE_BEARINGS  validation   \n",
       "1     ELECTRIC_DEMOLITION_HAMMERS  validation   \n",
       "2                    FISHING_RODS  validation   \n",
       "3                 PORCELAIN_TILES  validation   \n",
       "4  HAIR_SHAMPOOS_AND_CONDITIONERS  validation   \n",
       "\n",
       "                                     tokenized_title  \\\n",
       "0                                [metal, biela, hdi]   \n",
       "1  [repuestos, martillo, rotoprcutor, bosch, gshs...   \n",
       "2  [pesca, caña, pejerrey, colony, brava, fibra, ...   \n",
       "3        [porcelanato, abitare, ceramica, portinari]   \n",
       "4    [reconstruction, semi, lino, alfaparf, shampoo]   \n",
       "\n",
       "                                             data  target  n_labels     size  \n",
       "0                               [457, 1480, 3450]      88       632  1223820  \n",
       "1                    [3119, 892, 1, 767, 1, 9337]     174       632  1223820  \n",
       "2  [700, 990, 2057, 3990, 3670, 1737, 1153, 6568]     313       632  1223820  \n",
       "3                        [2722, 4404, 1406, 4405]     427       632  1223820  \n",
       "4                     [1, 3365, 7502, 10919, 849]     194       632  1223820  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7a0f27",
   "metadata": {},
   "source": [
    "Para el presente trabajo práctico solamente interesan las columnas **title** y **category**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e2a0a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Metal Biela Dw10 Hdi 2.0</td>\n",
       "      <td>ENGINE_BEARINGS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Repuestos Martillo Rotoprcutor Bosch Gshsce Po...</td>\n",
       "      <td>ELECTRIC_DEMOLITION_HAMMERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pesca Caña Pejerrey Colony Brava 3m Fibra De V...</td>\n",
       "      <td>FISHING_RODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Porcelanato Abitare Be 20x120 Cm. Ceramica Por...</td>\n",
       "      <td>PORCELAIN_TILES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reconstruction Semi Di Lino Alfaparf Shampoo 1...</td>\n",
       "      <td>HAIR_SHAMPOOS_AND_CONDITIONERS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                           Metal Biela Dw10 Hdi 2.0   \n",
       "1  Repuestos Martillo Rotoprcutor Bosch Gshsce Po...   \n",
       "2  Pesca Caña Pejerrey Colony Brava 3m Fibra De V...   \n",
       "3  Porcelanato Abitare Be 20x120 Cm. Ceramica Por...   \n",
       "4  Reconstruction Semi Di Lino Alfaparf Shampoo 1...   \n",
       "\n",
       "                         category  \n",
       "0                 ENGINE_BEARINGS  \n",
       "1     ELECTRIC_DEMOLITION_HAMMERS  \n",
       "2                    FISHING_RODS  \n",
       "3                 PORCELAIN_TILES  \n",
       "4  HAIR_SHAMPOOS_AND_CONDITIONERS  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['language', 'label_quality', 'split', 'tokenized_title', 'data', 'target', 'n_labels', 'size'],\n",
    "        inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56d54a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/validation_set.csv'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nom_archivo = './data/validation_set.csv'\n",
    "nom_archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3853245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'category'], dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = df.columns\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61d22ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(nom_archivo, header=list(cols), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b671769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En caso de quererlo, se reduce la muestra a n items.\n",
    "# Esta reducción es útil durante la confección del notebook, para ahorrar tiempo.\n",
    "n = 1000000\n",
    "df = df.sample(n, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b0f8da",
   "metadata": {},
   "source": [
    "### Etiquetar el target.\n",
    "\n",
    "A cada una de las 632 categorías del target se le asigna un valor entero entre 0 y 631. La relación entre la categoría u su etiqueta queda almacenada en un dataframe de Pandas llamado `df_categories`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e079aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df.category.unique()\n",
    "sorted_categories = np.sort(categories)\n",
    "df_categories = pd.DataFrame(sorted_categories, columns=['categories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29920e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categories['cat_tag'] = pd.DataFrame(list(range(df_categories.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c279da02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categories.set_index('categories', inplace=True)\n",
    "df_categories.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e1d327",
   "metadata": {},
   "source": [
    "## Construcción del Dataset\n",
    "\n",
    "El dataset se construye a partir de un dataframe de Pandas que debe tener al menos dos columnas:\n",
    "- **title**\n",
    "- **category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba04cc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeLiChallengeDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        item = {\n",
    "            \"data\": self.df.iloc[item][\"title\"],\n",
    "            \"target\": self.df.iloc[item][\"category\"]\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            item = self.transform(item)\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37e833f",
   "metadata": {},
   "source": [
    "## Preprocesamiento de los datos\n",
    "\n",
    "El preprocesamiento de texto tiene dos propósitos:\n",
    "- Tokenizar los títulos (datos) de modo que se quiten los signos de puntuación y palabras cortas como preposiciones y conjunciones (stopwords), todas las palabras queden en minúsculas, se separen en listas de palabras, etc.\n",
    "- Transformar las categorías en etiquetas numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13d206bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawDataProcessor:\n",
    "    def __init__(self, dataset, ignore_header=True, vocab_size=50000):\n",
    "        self.filters = [lambda s: s.lower(),\n",
    "                        preprocessing.strip_tags,\n",
    "                        preprocessing.strip_punctuation,\n",
    "                        preprocessing.strip_multiple_whitespaces,\n",
    "                        preprocessing.strip_numeric,\n",
    "                        preprocessing.remove_stopwords,\n",
    "                        preprocessing.strip_short]\n",
    "        \n",
    "        # Esta clase encapsula el mapeo entre las palabras normalizadas y sus correspondientes indices \n",
    "        # https://radimrehurek.com/gensim/corpora/dictionary.html\n",
    "        self.dictionary = corpora.Dictionary(\n",
    "            dataset[\"title\"].map(self._preprocess_string).tolist()\n",
    "        )\n",
    "        \n",
    "        # Filter the dictionary with extremos words\n",
    "        # https://tedboy.github.io/nlps/generated/generated/gensim.corpora.Dictionary.filter_extremes.html?highlight=filter_extrem\n",
    "        self.dictionary.filter_extremes(no_below=2, no_above=1, keep_n=vocab_size)\n",
    "        \n",
    "        # Asigna nuevos índices a todas las palabras\n",
    "        # https://tedboy.github.io/nlps/generated/generated/gensim.corpora.Dictionary.compactify.html\n",
    "        # https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.compactify\n",
    "        self.dictionary.compactify()\n",
    "        \n",
    "        # Se agregan tokens especiales\n",
    "        self.dictionary.patch_with_special_tokens({\"[PAD]\": 0,\n",
    "                                                   \"[UNK]\": 1})\n",
    "        \n",
    "        # Conversión de categorías a etiquetas\n",
    "        self.idx_to_target = sorted(dataset[\"category\"].unique())\n",
    "        self.target_to_idx = {t: i for i, t in enumerate(self.idx_to_target)}\n",
    "\n",
    "\n",
    "    def _preprocess_string(self, string):\n",
    "        # Procesamiento de los títulos mediante la aplicación de una lista de filtros\n",
    "        # Parámetro: str -> El título sin procesar\n",
    "        # Salida: list -> Lista de strings\n",
    "        # https://radimrehurek.com/gensim/parsing/preprocessing.html#gensim.parsing.preprocessing.preprocess_string\n",
    "        return preprocessing.preprocess_string(string, filters=self.filters)\n",
    "\n",
    "    def _sentence_to_indices(self, sentence):\n",
    "        # Convierte una lista de palabras en una lista de índices\n",
    "        # Parámetro: list -> Lista de palabras\n",
    "        # Salida: list -> Lista de enteros (índices) en el mismo orden que las palabras\n",
    "        # https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx\n",
    "        return self.dictionary.doc2idx(sentence, unknown_word_index=1)\n",
    "    \n",
    "    def encode_data(self, data):\n",
    "        # Convierte un string en una lista de índices\n",
    "        return self._sentence_to_indices(self._preprocess_string(data))\n",
    "    \n",
    "    def encode_target(self, target):\n",
    "        # Convierte las categorías a etiquetas\n",
    "        return self.target_to_idx[target]\n",
    "    \n",
    "    def __call__(self, item):\n",
    "        if isinstance(item[\"data\"], str):\n",
    "            data = self.encode_data(item[\"data\"])\n",
    "        else:\n",
    "            data = [self.encode_data(d) for d in item[\"data\"]]\n",
    "        \n",
    "        if isinstance(item[\"target\"], str):\n",
    "            target = self.encode_target(item[\"target\"])\n",
    "        else:\n",
    "            target = [self.encode_target(t) for t in item[\"target\"]]\n",
    "        \n",
    "        return {\n",
    "            \"data\": data,\n",
    "            \"target\": target\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68c66071",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadSequences:\n",
    "    def __init__(self, pad_value=0, max_length=None, min_length=1):\n",
    "        assert max_length is None or min_length <= max_length\n",
    "        self.pad_value = pad_value\n",
    "        self.max_length = max_length\n",
    "        self.min_length = min_length\n",
    "\n",
    "    def __call__(self, items):\n",
    "        data, target = list(zip(*[(item[\"data\"], item[\"target\"]) for item in items]))\n",
    "        seq_lengths = [len(d) for d in data]\n",
    "\n",
    "        if self.max_length:\n",
    "            max_length = self.max_length\n",
    "            seq_lengths = [min(self.max_length, l) for l in seq_lengths]\n",
    "        else:\n",
    "            max_length = max(self.min_length, max(seq_lengths))\n",
    "\n",
    "        data = [d[:l] + [self.pad_value] * (max_length - l)\n",
    "                for d, l in zip(data, seq_lengths)]\n",
    "            \n",
    "        return {\n",
    "            \"data\": torch.LongTensor(data),\n",
    "            \"target\": torch.FloatTensor(target)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "124b5563",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = RawDataProcessor(df)\n",
    "dataset = MeLiChallengeDataset(df, transform=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a804f035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El dataset tiene 1000000 elementos.\n",
      "Elemento #1000:\n",
      "\tData: [1178, 3084, 3085, 3080, 720, 3086]\n",
      "\tTarget: 297\n"
     ]
    }
   ],
   "source": [
    "i = 1000\n",
    "print(f\"El dataset tiene {len(dataset)} elementos.\")\n",
    "print(f\"Elemento #{i}:\\n\\tData: {dataset[i]['data']}\\n\\tTarget: {dataset[i]['target']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b720625c",
   "metadata": {},
   "source": [
    "## Carga del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e91fd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sequences = PadSequences()\n",
    "train_loader = DataLoader(dataset,\n",
    "                          batch_size=100,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=pad_sequences,\n",
    "                          drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4f7f30",
   "metadata": {},
   "source": [
    "## Construcción del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27fdf2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeLiChallengeClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 pretrained_embeddings_path, \n",
    "                 dictionary,\n",
    "                 vector_size,\n",
    "                 freeze_embedings):\n",
    "        super().__init__()\n",
    "        embeddings_matrix = torch.randn(len(dictionary), vector_size)\n",
    "        embeddings_matrix[0] = torch.zeros(vector_size)\n",
    "        with gzip.open(pretrained_embeddings_path, \"rt\") as fh:\n",
    "            for line in fh:\n",
    "                word, vector = line.strip().split(None, 1)\n",
    "                if word in dictionary.token2id:\n",
    "                    embeddings_matrix[dictionary.token2id[word]] =\\\n",
    "                        torch.FloatTensor([float(n) for n in vector.split()])\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings_matrix,\n",
    "                                                       freeze=freeze_embedings,\n",
    "                                                       padding_idx=0)\n",
    "        self.hidden1 = nn.Linear(vector_size, 128)\n",
    "        self.hidden2 = nn.Linear(128, 128)\n",
    "        self.output = nn.Linear(128, 632)\n",
    "        self.vector_size = vector_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274fe854",
   "metadata": {},
   "source": [
    "## Algoritmo de Optimización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65a15de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MeLiChallengeClassifier(ARCHIVO_DE_EMBEDDINGS, processor.dictionary, 50, True)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7772d8",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4dfe282f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e9777a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(EPOCHS):  # Recorre el dataset multiples veces\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for data in train_loader:\n",
    "        inputs = data['data'].to(device)\n",
    "        labels = data['target'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels.squeeze().long())\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f378dee4",
   "metadata": {},
   "source": [
    "## Evaluación del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e169eaaa",
   "metadata": {},
   "source": [
    "## Guardado de los parámetros del modelo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
